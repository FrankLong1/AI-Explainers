{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"mistralai/Mistral-7B-v0.1\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is about laying out the steps an LLM goes through to generate a response when it is fed a prompt. \n",
    "We assume some background on LLMs, but you don't need to be an engineer / know how to code to follow along.\n",
    "\n",
    "With that said you can copy and paste any of the code into ChatGPT and ask it to \"explain what this code does in plain English\" and it should do a solid job! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting Words into Tokens\n",
    "Key Terms:\n",
    "\"tokens\"\n",
    "\"embeddings\"\n",
    "\"matrices\"\n",
    "\n",
    "At their core, what machine learning models do is take in an input and predict what the most likely output is. W\n",
    "\n",
    "The tokenizer plays the key role of converting language understandable by humans (e.g. English), into the inputs that are understandable by the model (i.e. numbers, and ultimately 1s and 0s). \n",
    "\n",
    "Each model has its own accompanying Tokenizer that is trained alongside it, you can't think of it as a special converter that turns the language into this specific language\n",
    "\n",
    "NOTE NEED SOME NICE METAPHOR\n",
    "\n",
    "We'll start by loading a tokenizer from HuggingFace...\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The prompt we want to use is \"Can you write an email explaining what an LLM is?\" we'll start by defining this sentence for the model and giving it to the tokenizer to process..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT = \"Can you write an email explaining what an LLM is?\"\n",
    "inputs = tokenizer(PROMPT, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model breaks down the prompt into sub-word chunks called \"tokens\". What Large Language Models (LLMs) are trained to do is take in this string of \"tokens\", and perform \"next token prediction\" (i.e. guess the next token) based on the examples it has been \"trained on\" in the past."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Token Text</th>\n",
       "      <th>Token ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Can</td>\n",
       "      <td>2418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>you</td>\n",
       "      <td>368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>write</td>\n",
       "      <td>3324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>an</td>\n",
       "      <td>396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>email</td>\n",
       "      <td>4927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>explaining</td>\n",
       "      <td>20400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>what</td>\n",
       "      <td>767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>an</td>\n",
       "      <td>396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>LL</td>\n",
       "      <td>16704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>M</td>\n",
       "      <td>28755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>is</td>\n",
       "      <td>349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>?</td>\n",
       "      <td>28804</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Token Text  Token ID\n",
       "0          <s>         1\n",
       "1          Can      2418\n",
       "2          you       368\n",
       "3        write      3324\n",
       "4           an       396\n",
       "5        email      4927\n",
       "6   explaining     20400\n",
       "7         what       767\n",
       "8           an       396\n",
       "9           LL     16704\n",
       "10           M     28755\n",
       "11          is       349\n",
       "12           ?     28804"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tabulate import tabulate\n",
    "\n",
    "\n",
    "tokens_and_ids = []\n",
    "for i, token_id in enumerate(inputs[\"input_ids\"][0]):\n",
    "    token_text = tokenizer.decode([token_id])\n",
    "    tokens_and_ids.append((token_text, token_id.item()))\n",
    "\n",
    "\n",
    "df = pd.DataFrame(tokens_and_ids, columns=['Token Text', 'Token ID'])\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tokenizer knows which Token ID corresponds to what Token (i.e.\"sub-word chunk of text\"). The Token IDs are the numbers that the model actually uses to process the text. \n",
    "\n",
    "I am onlying showing the Token Text for explanation purposes!\n",
    "\n",
    "\n",
    "*Side Note: You'll also notice that the list of tokens begins with \"< s >\" which is the indicator to the model that a prompt or response is starting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I want to actually load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModel.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then here I move the model to the memory on a Nvidia GPU attached to my computer. You can think of this almost like copy and pasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\"\n",
    "model = model.to(device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
